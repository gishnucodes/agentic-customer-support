{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Hi! I can access Wikipedia to help answer your questions about history, science, people, places, or concepts - or we can just chat about anything else!\n",
      "(Type 'quit' to exit)\n",
      "\u001b[Knking... -\n",
      "Error chatting with the LM Studio server!\n",
      "\n",
      "Please ensure:\n",
      "1. LM Studio server is running at 127.0.0.1:1234 (hostname:port)\n",
      "2. Model 'llama-3.2-3b-instruct' is downloaded\n",
      "3. Model 'llama-3.2-3b-instruct' is loaded, or that just-in-time model loading is enabled\n",
      "\n",
      "Error details: Error code: 400 - {'error': \"Cannot put tools in the first user message when there's no first user message!\"}\n",
      "See https://lmstudio.ai/docs/basics/server for more information\n",
      "\u001b[Knking... -\n",
      "Error chatting with the LM Studio server!\n",
      "\n",
      "Please ensure:\n",
      "1. LM Studio server is running at 127.0.0.1:1234 (hostname:port)\n",
      "2. Model 'llama-3.2-3b-instruct' is downloaded\n",
      "3. Model 'llama-3.2-3b-instruct' is loaded, or that just-in-time model loading is enabled\n",
      "\n",
      "Error details: Error code: 400 - {'error': \"Cannot put tools in the first user message when there's no first user message!\"}\n",
      "See https://lmstudio.ai/docs/basics/server for more information\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LM Studio Tool Use Demo: Wikipedia Querying Chatbot\n",
    "Demonstrates how an LM Studio model can query Wikipedia\n",
    "\"\"\"\n",
    "\n",
    "# Standard library imports\n",
    "import itertools\n",
    "import json\n",
    "import shutil\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "\n",
    "# Third-party imports\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize LM Studio client\n",
    "client = OpenAI(base_url=\"http://127.0.0.1:1234/v1\", api_key=\"lm-studio\")\n",
    "MODEL = \"llama-3.2-3b-instruct\"\n",
    "\n",
    "\n",
    "def fetch_wikipedia_content(search_query: str) -> dict:\n",
    "    \"\"\"Fetches wikipedia content for a given search_query\"\"\"\n",
    "    try:\n",
    "        # Search for most relevant article\n",
    "        search_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "        search_params = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"list\": \"search\",\n",
    "            \"srsearch\": search_query,\n",
    "            \"srlimit\": 1,\n",
    "        }\n",
    "\n",
    "        url = f\"{search_url}?{urllib.parse.urlencode(search_params)}\"\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            search_data = json.loads(response.read().decode())\n",
    "\n",
    "        if not search_data[\"query\"][\"search\"]:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"No Wikipedia article found for '{search_query}'\",\n",
    "            }\n",
    "\n",
    "        # Get the normalized title from search results\n",
    "        normalized_title = search_data[\"query\"][\"search\"][0][\"title\"]\n",
    "\n",
    "        # Now fetch the actual content with the normalized title\n",
    "        content_params = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": normalized_title,\n",
    "            \"prop\": \"extracts\",\n",
    "            \"exintro\": \"true\",\n",
    "            \"explaintext\": \"true\",\n",
    "            \"redirects\": 1,\n",
    "        }\n",
    "\n",
    "        url = f\"{search_url}?{urllib.parse.urlencode(content_params)}\"\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            data = json.loads(response.read().decode())\n",
    "\n",
    "        pages = data[\"query\"][\"pages\"]\n",
    "        page_id = list(pages.keys())[0]\n",
    "\n",
    "        if page_id == \"-1\":\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"No Wikipedia article found for '{search_query}'\",\n",
    "            }\n",
    "\n",
    "        content = pages[page_id][\"extract\"].strip()\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"content\": content,\n",
    "            \"title\": pages[page_id][\"title\"],\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "\n",
    "# Define tool for LM Studio\n",
    "WIKI_TOOL = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"fetch_wikipedia_content\",\n",
    "        \"description\": (\n",
    "            \"Search Wikipedia and fetch the introduction of the most relevant article. \"\n",
    "            \"Always use this if the user is asking for something that is likely on wikipedia. \"\n",
    "            \"If the user has a typo in their search query, correct it before searching.\"\n",
    "        ),\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"search_query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Search query for finding the Wikipedia article\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"search_query\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# Class for displaying the state of model processing\n",
    "class Spinner:\n",
    "    def __init__(self, message=\"Processing...\"):\n",
    "        self.spinner = itertools.cycle([\"-\", \"/\", \"|\", \"\\\\\"])\n",
    "        self.busy = False\n",
    "        self.delay = 0.1\n",
    "        self.message = message\n",
    "        self.thread = None\n",
    "\n",
    "    def write(self, text):\n",
    "        sys.stdout.write(text)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    def _spin(self):\n",
    "        while self.busy:\n",
    "            self.write(f\"\\r{self.message} {next(self.spinner)}\")\n",
    "            time.sleep(self.delay)\n",
    "        self.write(\"\\r\\033[K\")  # Clear the line\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.busy = True\n",
    "        self.thread = threading.Thread(target=self._spin)\n",
    "        self.thread.start()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.busy = False\n",
    "        time.sleep(self.delay)\n",
    "        if self.thread:\n",
    "            self.thread.join()\n",
    "        self.write(\"\\r\")  # Move cursor to beginning of line\n",
    "\n",
    "\n",
    "def chat_loop():\n",
    "    \"\"\"\n",
    "    Main chat loop that processes user input and handles tool calls.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are an assistant that can retrieve Wikipedia articles. \"\n",
    "                \"When asked about a topic, you can retrieve Wikipedia articles \"\n",
    "                \"and cite information from them.\"\n",
    "            ),\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(\n",
    "        \"Assistant: \"\n",
    "        \"Hi! I can access Wikipedia to help answer your questions about history, \"\n",
    "        \"science, people, places, or concepts - or we can just chat about \"\n",
    "        \"anything else!\"\n",
    "    )\n",
    "    print(\"(Type 'quit' to exit)\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "        if user_input.lower() == \"quit\":\n",
    "            break\n",
    "\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        try:\n",
    "            with Spinner(\"Thinking...\"):\n",
    "                response = client.chat.completions.create(\n",
    "                    model=MODEL,\n",
    "                    messages=messages,\n",
    "                    tools=[WIKI_TOOL],\n",
    "                )\n",
    "\n",
    "            if response.choices[0].message.tool_calls:\n",
    "                # Handle all tool calls\n",
    "                tool_calls = response.choices[0].message.tool_calls\n",
    "\n",
    "                # Add all tool calls to messages\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"tool_calls\": [\n",
    "                            {\n",
    "                                \"id\": tool_call.id,\n",
    "                                \"type\": tool_call.type,\n",
    "                                \"function\": tool_call.function,\n",
    "                            }\n",
    "                            for tool_call in tool_calls\n",
    "                        ],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Process each tool call and add results\n",
    "                for tool_call in tool_calls:\n",
    "                    args = json.loads(tool_call.function.arguments)\n",
    "                    result = fetch_wikipedia_content(args[\"search_query\"])\n",
    "\n",
    "                    # Print the Wikipedia content in a formatted way\n",
    "                    terminal_width = shutil.get_terminal_size().columns\n",
    "                    print(\"\\n\" + \"=\" * terminal_width)\n",
    "                    if result[\"status\"] == \"success\":\n",
    "                        print(f\"\\nWikipedia article: {result['title']}\")\n",
    "                        print(\"-\" * terminal_width)\n",
    "                        print(result[\"content\"])\n",
    "                    else:\n",
    "                        print(\n",
    "                            f\"\\nError fetching Wikipedia content: {result['message']}\"\n",
    "                        )\n",
    "                    print(\"=\" * terminal_width + \"\\n\")\n",
    "\n",
    "                    messages.append(\n",
    "                        {\n",
    "                            \"role\": \"tool\",\n",
    "                            \"content\": json.dumps(result),\n",
    "                            \"tool_call_id\": tool_call.id,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                # Stream the post-tool-call response\n",
    "                print(\"\\nAssistant:\", end=\" \", flush=True)\n",
    "                stream_response = client.chat.completions.create(\n",
    "                    model=MODEL, messages=messages, stream=True\n",
    "                )\n",
    "                collected_content = \"\"\n",
    "                for chunk in stream_response:\n",
    "                    if chunk.choices[0].delta.content:\n",
    "                        content = chunk.choices[0].delta.content\n",
    "                        print(content, end=\"\", flush=True)\n",
    "                        collected_content += content\n",
    "                print()  # New line after streaming completes\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": collected_content,\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                # Handle regular response\n",
    "                print(\"\\nAssistant:\", response.choices[0].message.content)\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": response.choices[0].message.content,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"\\nError chatting with the LM Studio server!\\n\\n\"\n",
    "                f\"Please ensure:\\n\"\n",
    "                f\"1. LM Studio server is running at 127.0.0.1:1234 (hostname:port)\\n\"\n",
    "                f\"2. Model '{MODEL}' is downloaded\\n\"\n",
    "                f\"3. Model '{MODEL}' is loaded, or that just-in-time model loading is enabled\\n\\n\"\n",
    "                f\"Error details: {str(e)}\\n\"\n",
    "                \"See https://lmstudio.ai/docs/basics/server for more information\"\n",
    "            )\n",
    "            exit(1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat_loop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
