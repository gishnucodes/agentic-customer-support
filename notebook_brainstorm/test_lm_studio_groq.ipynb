{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Hi! I can access Wikipedia to help answer your questions about history, science, people, places, or concepts - or we can just chat about anything else!\n",
      "(Type 'quit' to exit)\n",
      "\u001b[Knking... -\n",
      "================================================================================\n",
      "\n",
      "Wikipedia article: Johnnie Walker (disambiguation)\n",
      "--------------------------------------------------------------------------------\n",
      "Johnnie Walker is a brand of whisky produced in Scotland.\n",
      "Johnnie, Johnny, or Jonny Walker may also refer to:\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Assistant: According to Wikipedia, Johnnie Walker is a brand of Scotch whisky that was first introduced in the early 19th century. The company is owned by Diageo and is one of the most recognizable and popular whisky brands in the world.\n",
      "\n",
      "The company was founded by John \"Johnnie\" Walker, who immigrated to Scotland from Ireland in the 1790s. He started working as a grocer's assistant and later set up his own business selling wine, whiskies, and other spirits.\n",
      "\n",
      "The famous \"Black Label\" Johnnie Walker whisky is one of the best-selling whiskies in the world and was first introduced in 1844. The brand is known for its iconic square bottle with a black label, which has remained largely unchanged over the years.\n",
      "\n",
      "Today, Johnnie Walker is one of Scotland's most iconic and successful whisky brands, with a wide range of products that cater to different tastes and preferences. (Source: Wikipedia Article on Johnnie Walker)\n",
      "\u001b[Knking... /\n",
      "Error chatting with the LM Studio server!\n",
      "\n",
      "Please ensure:\n",
      "1. LM Studio server is running at 127.0.0.1:1234 (hostname:port)\n",
      "2. Model 'llama-3.2-3b-instruct' is downloaded\n",
      "3. Model 'llama-3.2-3b-instruct' is loaded, or that just-in-time model loading is enabled\n",
      "\n",
      "Error details: Error code: 400 - {'error': 'Error in iterating prediction stream: StopIteration: '}\n",
      "See https://lmstudio.ai/docs/basics/server for more information\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LM Studio Tool Use Demo: Wikipedia Querying Chatbot\n",
    "Demonstrates how an LM Studio model can query Wikipedia\n",
    "\"\"\"\n",
    "\n",
    "# Standard library imports\n",
    "import itertools\n",
    "import json\n",
    "import shutil\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "\n",
    "# Third-party imports\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize LM Studio client\n",
    "client = OpenAI(base_url=\"http://127.0.0.1:1234/v1\", api_key=\"lm-studio\")\n",
    "MODEL = \"llama-3.2-3b-instruct\"\n",
    "\n",
    "\n",
    "def fetch_wikipedia_content(search_query: str) -> dict:\n",
    "    \"\"\"Fetches wikipedia content for a given search_query\"\"\"\n",
    "    try:\n",
    "        # Search for most relevant article\n",
    "        search_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "        search_params = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"list\": \"search\",\n",
    "            \"srsearch\": search_query,\n",
    "            \"srlimit\": 1,\n",
    "        }\n",
    "\n",
    "        url = f\"{search_url}?{urllib.parse.urlencode(search_params)}\"\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            search_data = json.loads(response.read().decode())\n",
    "\n",
    "        if not search_data[\"query\"][\"search\"]:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"No Wikipedia article found for '{search_query}'\",\n",
    "            }\n",
    "\n",
    "        # Get the normalized title from search results\n",
    "        normalized_title = search_data[\"query\"][\"search\"][0][\"title\"]\n",
    "\n",
    "        # Now fetch the actual content with the normalized title\n",
    "        content_params = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": normalized_title,\n",
    "            \"prop\": \"extracts\",\n",
    "            \"exintro\": \"true\",\n",
    "            \"explaintext\": \"true\",\n",
    "            \"redirects\": 1,\n",
    "        }\n",
    "\n",
    "        url = f\"{search_url}?{urllib.parse.urlencode(content_params)}\"\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            data = json.loads(response.read().decode())\n",
    "\n",
    "        pages = data[\"query\"][\"pages\"]\n",
    "        page_id = list(pages.keys())[0]\n",
    "\n",
    "        if page_id == \"-1\":\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"No Wikipedia article found for '{search_query}'\",\n",
    "            }\n",
    "\n",
    "        content = pages[page_id][\"extract\"].strip()\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"content\": content,\n",
    "            \"title\": pages[page_id][\"title\"],\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "\n",
    "# Define tool for LM Studio\n",
    "WIKI_TOOL = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"fetch_wikipedia_content\",\n",
    "        \"description\": (\n",
    "            \"Search Wikipedia and fetch the introduction of the most relevant article. \"\n",
    "            \"Always use this if the user is asking for something that is likely on wikipedia. \"\n",
    "            \"If the user has a typo in their search query, correct it before searching.\"\n",
    "        ),\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"search_query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Search query for finding the Wikipedia article\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"search_query\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# Class for displaying the state of model processing\n",
    "class Spinner:\n",
    "    def __init__(self, message=\"Processing...\"):\n",
    "        self.spinner = itertools.cycle([\"-\", \"/\", \"|\", \"\\\\\"])\n",
    "        self.busy = False\n",
    "        self.delay = 0.1\n",
    "        self.message = message\n",
    "        self.thread = None\n",
    "\n",
    "    def write(self, text):\n",
    "        sys.stdout.write(text)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    def _spin(self):\n",
    "        while self.busy:\n",
    "            self.write(f\"\\r{self.message} {next(self.spinner)}\")\n",
    "            time.sleep(self.delay)\n",
    "        self.write(\"\\r\\033[K\")  # Clear the line\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.busy = True\n",
    "        self.thread = threading.Thread(target=self._spin)\n",
    "        self.thread.start()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.busy = False\n",
    "        time.sleep(self.delay)\n",
    "        if self.thread:\n",
    "            self.thread.join()\n",
    "        self.write(\"\\r\")  # Move cursor to beginning of line\n",
    "\n",
    "\n",
    "def chat_loop():\n",
    "    \"\"\"\n",
    "    Main chat loop that processes user input and handles tool calls.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are an assistant that can retrieve Wikipedia articles. \"\n",
    "                \"When asked about a topic, you can retrieve Wikipedia articles \"\n",
    "                \"and cite information from them.\"\n",
    "            ),\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(\n",
    "        \"Assistant: \"\n",
    "        \"Hi! I can access Wikipedia to help answer your questions about history, \"\n",
    "        \"science, people, places, or concepts - or we can just chat about \"\n",
    "        \"anything else!\"\n",
    "    )\n",
    "    print(\"(Type 'quit' to exit)\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "        if user_input.lower() == \"quit\":\n",
    "            break\n",
    "\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        try:\n",
    "            with Spinner(\"Thinking...\"):\n",
    "                response = client.chat.completions.create(\n",
    "                    model=MODEL,\n",
    "                    messages=messages,\n",
    "                    tools=[WIKI_TOOL],\n",
    "                )\n",
    "\n",
    "            if response.choices[0].message.tool_calls:\n",
    "                # Handle all tool calls\n",
    "                tool_calls = response.choices[0].message.tool_calls\n",
    "\n",
    "                # Add all tool calls to messages\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"tool_calls\": [\n",
    "                            {\n",
    "                                \"id\": tool_call.id,\n",
    "                                \"type\": tool_call.type,\n",
    "                                \"function\": tool_call.function,\n",
    "                            }\n",
    "                            for tool_call in tool_calls\n",
    "                        ],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Process each tool call and add results\n",
    "                for tool_call in tool_calls:\n",
    "                    args = json.loads(tool_call.function.arguments)\n",
    "                    result = fetch_wikipedia_content(args[\"search_query\"])\n",
    "\n",
    "                    # Print the Wikipedia content in a formatted way\n",
    "                    terminal_width = shutil.get_terminal_size().columns\n",
    "                    print(\"\\n\" + \"=\" * terminal_width)\n",
    "                    if result[\"status\"] == \"success\":\n",
    "                        print(f\"\\nWikipedia article: {result['title']}\")\n",
    "                        print(\"-\" * terminal_width)\n",
    "                        print(result[\"content\"])\n",
    "                    else:\n",
    "                        print(\n",
    "                            f\"\\nError fetching Wikipedia content: {result['message']}\"\n",
    "                        )\n",
    "                    print(\"=\" * terminal_width + \"\\n\")\n",
    "\n",
    "                    messages.append(\n",
    "                        {\n",
    "                            \"role\": \"tool\",\n",
    "                            \"content\": json.dumps(result),\n",
    "                            \"tool_call_id\": tool_call.id,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                # Stream the post-tool-call response\n",
    "                print(\"\\nAssistant:\", end=\" \", flush=True)\n",
    "                stream_response = client.chat.completions.create(\n",
    "                    model=MODEL, messages=messages, stream=True\n",
    "                )\n",
    "                collected_content = \"\"\n",
    "                for chunk in stream_response:\n",
    "                    if chunk.choices[0].delta.content:\n",
    "                        content = chunk.choices[0].delta.content\n",
    "                        print(content, end=\"\", flush=True)\n",
    "                        collected_content += content\n",
    "                print()  # New line after streaming completes\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": collected_content,\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                # Handle regular response\n",
    "                print(\"\\nAssistant:\", response.choices[0].message.content)\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": response.choices[0].message.content,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"\\nError chatting with the LM Studio server!\\n\\n\"\n",
    "                f\"Please ensure:\\n\"\n",
    "                f\"1. LM Studio server is running at 127.0.0.1:1234 (hostname:port)\\n\"\n",
    "                f\"2. Model '{MODEL}' is downloaded\\n\"\n",
    "                f\"3. Model '{MODEL}' is loaded, or that just-in-time model loading is enabled\\n\\n\"\n",
    "                f\"Error details: {str(e)}\\n\"\n",
    "                \"See https://lmstudio.ai/docs/basics/server for more information\"\n",
    "            )\n",
    "            exit(1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat_loop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are crucial in various applications, including natural language processing (NLP), artificial intelligence (AI), and human-computer interaction. Their importance can be attributed to several factors:\n",
      "\n",
      "1. **Efficient Processing**: Fast language models can process large amounts of text data quickly, making them suitable for applications that require real-time or near-real-time processing, such as chatbots, virtual assistants, and language translation systems.\n",
      "2. **Improved User Experience**: By responding rapidly to user input, fast language models can enhance the overall user experience. Users can engage in more fluid and natural conversations, which can lead to increased satisfaction and loyalty.\n",
      "3. **Real-Time Applications**: Fast language models are essential for real-time applications, such as:\n",
      "\t* Sentiment analysis: quickly analyzing user feedback to identify trends and patterns.\n",
      "\t* Named entity recognition: rapidly extracting specific information from text data, such as names, locations, and organizations.\n",
      "\t* Machine translation: translating text in real-time, allowing for seamless communication across languages.\n",
      "4. **Large-Scale Data Analysis**: Fast language models can efficiently process and analyze massive datasets, enabling applications such as:\n",
      "\t* Text summarization: quickly condensing large documents into concise summaries.\n",
      "\t* Topic modeling: identifying patterns and themes in large text corpora.\n",
      "\t* Text classification: categorizing text into predefined categories, such as spam vs. non-spam emails.\n",
      "5. **Cost Savings**: Fast language models can reduce the computational resources required for NLP tasks, resulting in:\n",
      "\t* Lower energy consumption: faster models can process data more efficiently, reducing the need for powerful hardware and minimizing energy costs.\n",
      "\t* Reduced infrastructure costs: faster models can be deployed on less powerful hardware, reducing the need for expensive infrastructure upgrades.\n",
      "6. **Competitive Advantage**: Organizations that leverage fast language models can gain a competitive advantage by:\n",
      "\t* Responding quickly to customer inquiries and concerns.\n",
      "\t* Analyzing large datasets to identify trends and patterns, enabling data-driven decision-making.\n",
      "\t* Developing more efficient and effective language-based applications.\n",
      "7. **Research and Development**: Fast language models accelerate research and development in NLP, enabling researchers to:\n",
      "\t* Explore new ideas and techniques more quickly.\n",
      "\t* Evaluate the performance of different models and architectures more efficiently.\n",
      "\t* Develop more advanced and accurate language models.\n",
      "\n",
      "In summary, fast language models are essential for a wide range of applications, from real-time processing and efficient data analysis to cost savings and competitive advantage. As the demand for NLP-powered applications continues to grow, the importance of fast language models will only continue to increase.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
